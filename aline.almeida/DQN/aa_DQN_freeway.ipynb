{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "aa_DQN_freeway.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "clJkwoAG6Phk",
        "wskzQX8N6Phl",
        "86j84qlA6Phv"
      ],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlmeidaAlin3/AI-Synthesized_Voices_Clusterization/blob/main/aline.almeida/DQN/aa_DQN_freeway.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRj5nWuM6PhK"
      },
      "source": [
        "# Freeway"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKMotWp56PhY"
      },
      "source": [
        "This is the **second project** for the MC935rA/MO436A - Reinforcement Learning course, taught by Prof. Esther Colombini.\n",
        "\n",
        "In this project we propose to apply Deep Reinforcement Learning methods to teach an agent how to play the Freeway Atari game."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exKbCcw26Pha"
      },
      "source": [
        "**Group members:**\n",
        "- Aline Gabriel de Almeida\n",
        "- Dionisius Oliveira Mayr (229060)\n",
        "- Leonardo de Oliveira Ramos (171941)\n",
        "- Marianna de Pinho Severo (264960)\n",
        "- Victor Jes√∫s Sotelo Chico (265173)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmyFZKMI6Phb"
      },
      "source": [
        "## Freeway game\n",
        "\n",
        "<center><img src=https://raw.githubusercontent.com/DionisiusMayr/FreewayGame/main/freeway/img/Freeway_logo.png>\n",
        "</center>\n",
        "\n",
        "Freeway is a video game written by David Crane for the Atari 2600 and published by Activision [[1]](https://en.wikipedia.org/wiki/Freeway_(video_game)).\n",
        "\n",
        "In the game, two players compete against each other trying to make their chickens cross the street, while evading the cars passing by.\n",
        "There are three possible actions: staying still, moving forward or moving backward.\n",
        "Each time a chicken collides with a car, it is forced back some spaces and takes a while until the chicken regains its control.\n",
        "\n",
        "When a chicken is successfully guided across the freeway, it is awarded one point and moved to the initial space, where it will try to cross the street again.\n",
        "The game offers multiple scenarios with different vehicles configurations (varying the type, frequency and speed of them) and plays for 2 minutes and 16 seconds.\n",
        "During the 8 last seconds the scores will start blinking to indicate that the game is close to end.\n",
        "Whoever has the most points after this, wins the game!\n",
        "\n",
        "The image was extracted from the [manual of the game](https://www.gamesdatabase.org/Media/SYSTEM/Atari_2600/Manual/formated/Freeway_-_1981_-_Zellers.pdf).\n",
        "\n",
        "[1 - Wikipedia - Freeway](https://en.wikipedia.org/wiki/Freeway_(video_game))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oz1wIiA6Phc"
      },
      "source": [
        "# Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okeePgwz6Phd"
      },
      "source": [
        "We will be using the [Stable Baselines](https://stable-baselines.readthedocs.io/en/master/index.html) toolkit.\n",
        "This toolkit is a set of improved implementations of Reinforcement LEarning algorithms based on [OpenAI Baselines](https://github.com/openai/baselines).\n",
        "\n",
        "Although the game offers multiple scenarios, we are going to consider only the first one. Also, we will be controlling a *single chicken*, while we try to maximize its score.\n",
        "\n",
        "In this configuration, there are ten lanes and each lane contains exactly one car (with a different speed and direction).\n",
        "Whenever an action is chosen, it is repeated for $k$ frames, $k \\in \\{2, 3, 4\\}$.\n",
        "\n",
        "This means that our environment is **stochastic** and it is also **episodic**, with its terminal state being reached whenever 2 minutes and 16 seconds have passed.\n",
        "\n",
        "You can find more information regarding the environment used at [Freeway-ram-v0](https://gym.openai.com/envs/Freeway-v0/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abh0tSP06Phe"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzsgJhjl6Phe"
      },
      "source": [
        "Install the dependencies:\n",
        "```sh\n",
        "pip install -r requirements.txt\n",
        "pip install stable-baselines\n",
        "pip install tensorflow==1.15.0\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRlNBUOE6Phf"
      },
      "source": [
        "# Useful Resources"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUeKMfnq6Phf"
      },
      "source": [
        "Here you can find a list of useful links and materials that were used during this project.\n",
        "\n",
        "* [Freeway-ram-v0 from OpenAI Gym](https://gym.openai.com/envs/Freeway-ram-v0/)\n",
        "* [Manual of the game](https://www.gamesdatabase.org/Media/SYSTEM/Atari_2600/Manual/formated/Freeway_-_1981_-_Zellers.pdf)\n",
        "* [Freeway Disassembly](http://www.bjars.com/disassemblies.html)\n",
        "* [Atari Ram Annotations](https://github.com/mila-iqia/atari-representation-learning/blob/master/atariari/benchmark/ram_annotations.py)\n",
        "* [Freeway Benchmarks](https://paperswithcode.com/sota/atari-games-on-atari-2600-freeway)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iSwfr-M6Phg"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ayz1Hcu6Phg"
      },
      "source": [
        "import sys\n",
        "sys.path.append('../')  # Enable importing from `src` folder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cjd1pUS6Phh"
      },
      "source": [
        "%matplotlib inline\n",
        "import statistics\n",
        "from collections import defaultdict\n",
        "from functools import lru_cache\n",
        "from typing import List\n",
        "\n",
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import src.agents as agents\n",
        "import src.episode as episode\n",
        "import src.environment as environment\n",
        "import src.aux_plots as aux_plots\n",
        "import src.serializer as serializer\n",
        "import src.gif as gif\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import gym\n",
        "from stable_baselines.common import make_vec_env\n",
        "from stable_baselines.common.cmd_util import make_atari_env\n",
        "from stable_baselines.common.vec_env import VecFrameStack\n",
        "from stable_baselines.common.atari_wrappers import make_atari\n",
        "\n",
        "from stable_baselines.deepq.policies import CnnPolicy\n",
        "from stable_baselines import DQN"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6sa_1jGr6Phi"
      },
      "source": [
        "def print_result(i, scores, total_reward, score):\n",
        "    if i % 10 == 0:\n",
        "        print(f\"Run [{i:4}] - Total reward: {total_reward:7.2f} Mean scores: {sum(scores) / len(scores):.2f} Means Scores[:-10]: {sum(scores[-10:]) / len(scores[-10:]):5.2f} Score: {score:2} \")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "595kmf0y6Phj"
      },
      "source": [
        "def read_int_array_from_file(fn: str):\n",
        "    with open(f\"./experiments/{fn}\") as f:\n",
        "        return [int(x) for x in f.read().splitlines()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_by9we2w6Phj"
      },
      "source": [
        "# Action space"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BA0HBFbt6Phj"
      },
      "source": [
        "As we said above, the agent in this game has three possible actions at each frame, each represented by an integer:\n",
        "\n",
        "* 0: Stay\n",
        "* 1: Move forward\n",
        "* 2: Move backward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tkfPTiC6Phk"
      },
      "source": [
        "# Baseline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clJkwoAG6Phk"
      },
      "source": [
        "## State of the art benchmarks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-wLm9gz6Phk"
      },
      "source": [
        "The image bellow (extracted from https://paperswithcode.com/sota/atari-games-on-atari-2600-freeway) shows the evolution of the scores over time using different techniques.\n",
        "\n",
        "Today, the state of the art approaches are making 34.0 points, using Deep Reinforcement Learning methods."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-p9lBOP6Phl"
      },
      "source": [
        "<center><img src=https://raw.githubusercontent.com/DionisiusMayr/FreewayGame/47d8d1fd3b921471738b30b5f9ae447593705b09/freeway/img/state_of_art_scores.png>\n",
        "</center>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wskzQX8N6Phl"
      },
      "source": [
        "## Simple baseline agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgO5OARX6Phm"
      },
      "source": [
        "As a simple baseline, we are using an agent that always moves **up**, regardless of the rewards received or the current state."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZNYJOaO6Phn"
      },
      "source": [
        "env, initial_state = environment.get_env()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bM3XH1kp6Pho"
      },
      "source": [
        "agent = agents.Baseline()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1CFZpBp6Pho"
      },
      "source": [
        "total_rewards = []\n",
        "n_runs = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9N0N3Qb6Pho",
        "outputId": "418e1f45-3b7e-4260-a862-f7936c993c51"
      },
      "source": [
        "%%time\n",
        "for i in range(n_runs):\n",
        "    render = i % 10 == 0\n",
        "\n",
        "    game_over = False\n",
        "    state = env.reset()\n",
        "    action = agent.act(state)\n",
        "\n",
        "    total_reward = 0\n",
        "\n",
        "    while not game_over:\n",
        "        if render:\n",
        "            time.sleep(0.01)\n",
        "            env.render()\n",
        "\n",
        "        ob, reward, game_over, _ = env.step(action)\n",
        "\n",
        "        total_reward += reward\n",
        "        action = agent.act(state)  # Next action\n",
        "\n",
        "    total_rewards.append(total_reward)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 21.5 s, sys: 544 ms, total: 22.1 s\n",
            "Wall time: 50 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlGDKdlX6Phr",
        "outputId": "7f5f0335-342a-4a81-e9b0-0b9a4eb93a35"
      },
      "source": [
        "total_rewards"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[23.0, 21.0, 23.0, 21.0, 21.0, 21.0, 23.0, 21.0, 23.0, 21.0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0AYe3QAg6Phs",
        "outputId": "22e23242-3e5a-4245-fa37-61d5f0fa58ff"
      },
      "source": [
        "baseline_mean_score = np.mean(total_rewards)\n",
        "baseline_mean_score"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "21.8"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tkJKCMe6Pht"
      },
      "source": [
        "As we can see, this agent usually scores 21 or 23 points (as shown in the images bellow). It depends on the the values of $k$ sampled, and on average it scores about 21.8 points per run."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2kdIiIK6Phu"
      },
      "source": [
        "<center><img src=https://raw.githubusercontent.com/DionisiusMayr/FreewayGame/47d8d1fd3b921471738b30b5f9ae447593705b09/freeway/img/baseline_1.png>\n",
        "</center>\n",
        "\n",
        "<center><img src=https://raw.githubusercontent.com/DionisiusMayr/FreewayGame/47d8d1fd3b921471738b30b5f9ae447593705b09/freeway/img/baseline_2.png>\n",
        "</center>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEY3MfGB6Phu"
      },
      "source": [
        "# State Representation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMBFgeor6Phv"
      },
      "source": [
        "Since the tabular methods we are going to use work with some representation of the actual environment state, we will need to understand it better in order to effectively approach this problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86j84qlA6Phv"
      },
      "source": [
        "## Atari 2600"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMyZP5rt6Phv"
      },
      "source": [
        "Before talking about the state representation, it is important to understand how the Atari 2600 works.\n",
        "\n",
        "Atari 2600 is a video game released in 1977 by the American Atari, Inc.\n",
        "Its **8-bit** microprocessor was of the MOS **6502** family and it had **128 bytes** of RAM.\n",
        "\n",
        "And these 128 bytes are what really matters here.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpZ-PQ-16Phw"
      },
      "source": [
        "Recall that Gym gives us the RAM memory of the Atari as the state representation.\n",
        "In other words, it gives us an 128-element `np.array`, where each element of the array is an `uint8` (*integer values ranging from 0 to 255*).\n",
        "\n",
        "That said, we have (in theory) $256^{128} \\approx 1.8 \\cdot 10^{308}$ possible game states!\n",
        "\n",
        "This is *far* from being manageable, and thus we need to come up with a different approch to represent our state if we want our algorithms to converge.\n",
        "\n",
        "One might argue that the RAM state is *sparse* and although that is true, it is still not sparse enough to apply tabular methods."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ji7ogEuc6Ph3"
      },
      "source": [
        "# Reward Policy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljfENGRz6Ph3"
      },
      "source": [
        "In the base environment we are awarded on point each time we successfully cross the freeway."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucF4x5xK6Ph5"
      },
      "source": [
        "# Hyper Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VQBJpkZ6Ph5"
      },
      "source": [
        "GAMMA = 0.99\n",
        "LEARNING_RATE = 0.0005\n",
        "EXPLORATION_RATE = 0.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6BzspnJ6Ph6"
      },
      "source": [
        "# Methodology"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t36obrBM6Ph6"
      },
      "source": [
        "Since it takes a lot of time to train the models, we won't train them all in this report.\n",
        "Instead, we will be load the results of our simulations and specifying the parameters used to obtain those results.\n",
        "Of course, it is possible to reproduce our results simply by running the algorithms here using the same hyper parameters as specified.\n",
        "\n",
        "Whenever possible, we will be adding plots comparing different approaches and parameters, as well as adding gifs in this notebook so that we can visualize the development of the agent and unique strategies that they learned."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rZEku736Ph6"
      },
      "source": [
        "Also, we focused a lot of our experiments on Q-Learning, since it was showing the most promissor results.\n",
        "Monte Carlo methods didn't really work out, and SARSA($\\lambda$) methods took way too much time to run (roughly 12 hours per 2k iterations!).\n",
        "Since QLearning and SARSA aren't really that different, we applied most of the knowledge we acquired from the QLearning experiments on SARSA, varying only its unique parameter, $\\lambda$, in steps of 0.2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "poesED2Q6Ph6"
      },
      "source": [
        "Also, it is worth mentioning that we left the code used by each agent inside `./src/agents.py` and provided a model of implementing the environment along the notebook, with the `n_runs` parameter (that controls the number of episodes used in to train the algorithm) set to `1`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uuS4JS46Ph7"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfA13AxhB0zJ"
      },
      "source": [
        "# DQN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQHaobajDecf"
      },
      "source": [
        "The [Deep-Q-Network](https://arxiv.org/pdf/1312.5602.pdf) is a deep learning model that learns to control policies directly from high dimensional sensory using reinforcement learning.  \n",
        "\n",
        "The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating the future rewards.  \n",
        "\n",
        "We applied the [algorithm implemented by Stable Baselines](https://stable-baselines.readthedocs.io/en/master/modules/dqn.html) to the Atari Freeway game.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-m6j7vatDej1"
      },
      "source": [
        "The Deep-Q-Network algorithm observes the image $x_t$ from the emulator which is a vector of raw pixel values representing the current screen. In addition it receives a reward $r_t$ representing the change in game score.\n",
        "\n",
        "It considers sequences of actions and observations, $s_t = x_1, a_1, x_2, ... a_{t-1}, x_t$, and learn game strategies that depend upon these sequences.  \n",
        "\n",
        "All sequences in the emulator are assumed to terminate in a finite number of time-steps. This formalism gives rise to a large but finite Markov decision process (MDP) in which each sequence is a distinct state.  \n",
        "\n",
        "As a result, we can apply standard reinforcement learning methods for MDPs, simply by using the complete sequence $s_t$ as the state representation at time $t$.  \n",
        "\n",
        "The future reward are discounted by a factor of $\\gamma$ per time-step.  \n",
        "\n",
        "\n",
        "The optimal action-value function obeys an important identity known as the Bellman equation. This is based on the following intuition: if the optimal value $Q*(s', a')$ of the sequence $s'$ at the next time-step was known for all possible actions $a'$, then the optimal strategy is to select the action $a'$\n",
        "maximising the expected value of $r + \\gamma Q*(s', a')$:  \n",
        "  \n",
        "  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f51teduuFGgG"
      },
      "source": [
        "$Q*(s, a) = E_{s' ~ \\epsilon}[r + \\gamma max_{a'}Q*(s', a')|s, a]$  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqMEYj6VFGnZ"
      },
      "source": [
        "A Q-network can be trained by minimising a sequence of loss functions $L_i(\\theta_i)$ that changes at each iteration $i$:  \n",
        "\n",
        "$L_i(\\theta_i) = E_{s, a ~p(.)}[(y_i - Q(s, a; \\theta_i))^2$ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfBSvtxDFGtW"
      },
      "source": [
        "where  \n",
        "\n",
        "$y_i = E_{s' ~ \\epsilon}[r + \\gamma max_{a'}Q*(s', a'; \\theta_i)|s,a] $   \n",
        "\n",
        "is the target for iteration $i$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rh05DhCt6Ph-"
      },
      "source": [
        "## Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4zV0AFhwfKR"
      },
      "source": [
        "### Influence of the discount factor $\\gamma$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stvc_pWT6PiH"
      },
      "source": [
        "The discount factor $\\gamma$ determines how much the agent cares about rewards in the distant future relative to those in the immediate future.  \n",
        "\n",
        "If $\\gamma$=0, the agent will be completelly myopic and only learn about actions that produce an immediate reward.If $\\gamma$=1, the agent will evaluate each of its actions based on the sum of total of all futures rewards.\n",
        "\n",
        "We used a $\\gamma$ value of 0.99 in order to make our agent care about distant future and we also decreased this value to 0.90 and 0.75 to see how they can impact the agent behavior. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tP2-TKkX6PiI"
      },
      "source": [
        "Thus, we will be experimenting with 3 different parameters set:\n",
        "\n",
        "| Parameter | G1 | G2 | G3 |\n",
        "|------|----|----|----|\n",
        "| **`GAMMA`** | 0.99 | 0.90 | 0.75 |\n",
        "| `LEARNING_RATE` | 0.0005 | 0.0005 | 0.0005 |\n",
        "| `EXPLORATION_RATE` | 0.1 | 0.1 | 0.1 |\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bp79q2EdGls"
      },
      "source": [
        "#### IMAGEM DO GRAFICO AQUI ####"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LhUBXEXudXUn"
      },
      "source": [
        "# From the plots above we can see that $\\gamma = 0.75$ led to poor results, but $\\gamma = 0.9$ and $\\gamma = 0.99$ seems to be equivalent.\n",
        "# An explanation is that when we make our agent short-sighted, it doesn't try to cross all the lanes and receive that huge reward we are offering as much as the far-sighted agents try.\n",
        "\n",
        "# That being said, we will be focusing on the $\\gamma = 0.99$, arbitrarily.\n",
        "# We could be using the $0.9$ too, since it appears to have the same performance."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqRy84cAdjUI"
      },
      "source": [
        "### Influence of the learning rate parameter\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0E9TEWNpdsx2"
      },
      "source": [
        "We will be experimenting with 3 different parameters set:\n",
        "\n",
        "| Parameter | G1 | G2 | G3 |\n",
        "|------|----|----|----|\n",
        "| `GAMMA` | 0.99 | 0.99 | 0.99 |\n",
        "| **`LEARNING_RATE`** | 0.0005 | 0.0010 | 0.0050 |\n",
        "| `EXPLORATION_RATE` | 0.1 | 0.1 | 0.1 |"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHJ26scHdoud"
      },
      "source": [
        "#### IMAGEM DO GRAFICO AQUI ####"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cv_3dIkDdpWS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xiogqiuFwfKN"
      },
      "source": [
        "### Influence of the agent's exploration rate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDD_PMkQ6PiL"
      },
      "source": [
        "The exploration rate is the probability that our agent will explore the environment rather than exploit it.  \n",
        "\n",
        "We used 0.1 as our baseline exploration value. In order to see how the exploration rate impact the agent behavior, we also made experiments using the double of this value (0.1) and the half of it (0.05)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "padSZjle6PiL"
      },
      "source": [
        "All in all, these are the parameters that we are going to use to execute this experiment.\n",
        "\n",
        "| Parameter | G1 | G2 | G3 |\n",
        "|------|----|----|----|\n",
        "| `GAMMA` | 0.99 | 0.99 | 0.99 |\n",
        "| `LEARNING_RATE` | 0.0005 | 0.0005 | 0.0005 |\n",
        "| **`EXPLORATION_RATE`** | 0.1 | 0.05 | 0.20 |"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jp93T-TjeHWy"
      },
      "source": [
        "#### IMAGEM DO GRAFICO AQUI ####"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Imn2a7GKeHbI"
      },
      "source": [
        "# The exploration rate is the probability that our agent will explore the environment rather than exploit it.  \n",
        "\n",
        "# As we can see from the results show in the plots above, the lower is the $N0$ value, the better is the performance of the agent.\n",
        "# Although this migth seem counterintuitive at first, in fact, it stands to reason.\n",
        "# When we explore more (higher $N0$), we exploit less, leading to worst results in the beginning.\n",
        "# From the graphs above, we can see that all three lines are looking up, still increasing their values, and the gap between them is closing.\n",
        "# We expect to achive better results with higher $N0$s, but it would take too much time for it to happen (we even tested some of them overnight and it still wasn't enough).\n",
        "\n",
        "# Based on our reward function, it is fairly simple to detect which action should be taken in most of the states.\n",
        "# We want to move up always, unless it is leading to a collision.\n",
        "# Thus, frequently it is easy to detect the best action, and for most of the states we don't need to explore a lot to find it."
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}